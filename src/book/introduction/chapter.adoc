[[chapter_what_is]]
== User Journey Testing and the Test Spectrum

[.lead]
Testing has been important to me as a professional software engineer for as long as I can remember. 

Now, I'm not suggesting that I've been writing tests since the very beginning, or that I always write tests, or even that the tests I write are particularly good. But my first professional development gigs as a software consultant were in Java in the late 1990s, and a new project called _JUnit_ -- a unit testing framework written by Erich Gamma and Kent Beck -- was really taking off. 

What intrigued me about JUnit at the time was that it was not just a simple testing library -- it was a key part of a philosophy called _Test-Driven Development_ (*TDD*), which in turn was a key part of a larger system of agile practices called _Extreme Programming_ (*XP*). Not surprisingly, Kent Beck (along with co-author Martin Fowler) wrote a book about all of these practices called _Planning Extreme Programming_ -- one of the first books I read on the subject.

Despite the "Extreme" qualifier in XP, the practices recommended by Kent and Martin in their book seemed quite sensible and practical: 

* Customers pick the features to be added
* Programmers add the features so that they are completely ready to be used
* Programmers and customers write and maintain automated tests to demonstrate the presence of these features

This felt like such a common sense strategy to me at the time that I couldn't fully grasp why all software developers didn't use this approach. If I drop my car off at the repair shop and say, "When I drive above 55 miles per hour, I hear a loud clanking", I fully expect the mechanic to:

* Drive my car above 55 miles per hour so that they can hear (and verify) the clanking sound
* Fix the clanking
* Demonstrate to me, when I pick up my car after the repair, that the clanking is gone by driving above 55 miles per hour with me in the car

Now, if you've been programming for a while, you might be thinking, "That clanking is a bug, not a feature!" And while you're technically correct, what different behavior would you expect if I dropped my car off and said instead, "I'd like you to upgrade my sound system" or "I'd like you to install a new sun roof"? I'd expect the same sequence of events. Wouldn't you?

=== Introducing Taiko

Suppose my client says to me, "I need a website for a software conference I'm running. I'd like to have a page that lists all of the speakers. When you click on a speaker, I'd like that to lead to a page with their biography and a list of their talks." What they just described to me is a _user journey_. 

So, I understand the feature they're asking for. I can add that feature with relatively little effort. But how can I demonstrate the new feature I just added?

As the developer of the feature, I probably manually go through the sequential steps of "Go to the Speakers Page; Click on a Speaker; Verify that I end up on a page with the Speaker's biography and list of talks" tens, if not hundreds, of times during the development process. After all, I want to be fully convinced that the process works before I demonstrate it to my client. 

But manual testing can be time consuming and prone to error if not done consistently. What if I could automate the user journey? What if I could write a little bit of code that tests the user journey in a consistent, repeatable manner? Something like this:

[code, javascript]
.A user journey test written in Taiko
----
openBrowser()
goto('https://thirstyhead.com/conferenceworks/speakers/')
click('Dr. Rebecca Parsons')
highlight('About')
highlight('Talks')
screenshot({path:'speakerListTest-screenshot.png'})
----

.The resulting screenshot from the user journey test
image::introduction/speakerListTest-screenshot.png[Screenshot from the user journey test]

_Taiko_ is an open source Node.js library for testing modern web applications. It is a purpose-built _Domain Specific Language_ (*DSL*) for writing user journey tests. Anything that your user can do on your website can be automated using Taiko. So, if your user does this on their _login journey_:   

* Go to the Login page
* Click the 'Username' field
* Write in the user name
* Click the 'Password' field
* Write in the password
* Click the Submit button 

...you can automate that with Taiko like this:

[code, javascript]
.Automating the login user journey with Taiko
----
openBrowser()
goto('https://thirstyhead.com/conferenceworks/login')
click('Username')
write('suzi@q.com')
click('Password')
write('wordpass')
click('Submit')
----

Of course, TDD is no more tied to JUnit than user journey testing is tied to Taiko. User journey testing is a practice -- a discipline -- that can be implemented in a variety of different languages, using a variety of different libraries. If you can practice TDD by using a library other than JUnit (say, _NUnit_ for .NET languages, or _Test::Unit_ for Ruby), then you can certainly write user journey tests using a library other than Taiko. But I'll continue to use Taiko here whenever I need to explain a concept in code.

If you'd like to follow along and run the Taiko tests yourself, installing Taiko is as simple as typing `npm install -g taiko`. Once Taiko is installed, you can type `taiko` at the command prompt to enter the interactive REPL and explore on your own. Anything that you type in the Taiko REPL can be exported to modern JavaScript by typing `.code` to see the code on screen, or by typing `.code mytest.js` to save the JavaScript to the current working directory. After that, you can type `taiko mytest.js` to run the code outside of the REPL by hand or, say, in your _Continuous Delivery_ (*CD*) pipeline.  

=== Reconsidering the Test Pyramid

One of the most important aspects of unit testing is, well, the _unit_ of code being tested. More specifically, the size of the unit. The goal of unit testing is to focus on the smallest cohesive hunk of code that you can tease apart from the rest of the application in isolation. I often say that unit tests "test the bricks, not the building" because, after all, you can't trust the building if you don't trust the bricks. 

If your unit of code interacts with a database, or a file system, or a remote web service, it's common to _mock_ or _stub_ out those services with a fast, in-memory doppelg√§nger that behaves just like the original service does, but without the latency and brittleness that depending on an external service might introduce.  

Author Mike Cohn, in his book _Succeeding with Agile_, introduced a powerful visual metaphor for this with the _Test Pyramid_.

.Mike Cohn's Test Pyramid, from _Succeeding with Agile_
image::introduction/ui-test-pyramid@2x.png[Mike Cohn's Test Pyramid]

Under this rubric, developers are encouraged to write as many unit tests as they can, because they are the fastest, most stable, most repeatable types of tests. Service Tests (or, more popularly in subsequent years, _Integration Tests_, since they integrate with the databases and web services that unit tests intentionally mock out) still offer value, but they are typically slower to run, and more prone to brittleness due to circumstances beyond the control of the test itself. According to the Test Pyramid, you should write fewer of them than unit tests.

At the tip of the pyramid are _UI Tests_ (User Interface Tests). Since all of the pieces of the application must be up and running, properly configured and secured -- and since many of those services may be out of the immediate control of the individual developer, or there may be a lack of a proper testing environment that accurately mirrors the production environment -- these tests are visually deemed "least important" in the hierarchy of tests.

Arguing against the validity of the Test Pyramid, even in a book that is all about those tests that live on the vanishing tip of the pyramid as it recedes from view, is a futile battle. Especially since I personally agree with the message of the Test Pyramid -- that is, if I'm a developer who is sitting furthest away from that mythical _user_ that everyone else seems to insist exists. Writing a test on behalf of someone who I most likely will never meet is a tall order to fill. On the other hand, writing tests for my fellow developers -- developers who I deal with every day; developers who will be depending on the validity of my code so that they can trust in the validity of their own code -- is a crucial and essential goal.  

This myopic view of the development process as a whole isn't myopic in the least if you're a brick builder. But everyone else actively involved in the process who is further "up the pyramid", towards the user and the finished software product, might take issue with their role (and their tests) being deemed "less important".  

=== Introducing the Test Spectrum

Consider, for a moment, the legion of software development professionals who deal with the user directly and repeatedly. The group of software developers who are just as dedicated to the validity of the software application being developed. The group of professionals who want to apply the same engineering rigor of testing to the _User Experience_ as thoughtful developers do to the _Developer Experience_.  

This change in perspective might benefit from a different visual metaphor. Let's call it the _Test Spectrum_.

.A new visual metaphor for software development that places the app at the center of focus: the Test Spectrum
image::introduction/dx-ux-empty@2x.png[The Test Spectrum]

First of all, let's place the application at the center of our model. A finished, correctly working app is the highest priority of both the developer and the user. As the user describes what they want the app to do, the developer converts their vision into working code. The application, therefore, is both the fulcrum of the user-developer relationship as well as proof of its success.

The application is also an opaque boundary between the two worlds. Source code, and the test suite that measures its success, is quite literally written in a foreign, unintelligible language to the end user. A symphonygoer can tell you in great detail what they enjoy about the music, but they may or may not be able to point to the specific passage in the sheet music that brings them such joy.   

So, with this new perspective in mind, let's place unit tests on the Test Spectrum.

.Unit tests on the Test Spectrum
image::introduction/dx-ux-1@2x.png[Unit tests on the Test Spectrum]

In our new visual metaphor, we can see that unit tests are about as far away from the User Experience as the spectrum allows. This doesn't mean that unit tests are unimportant; instead, it shows us who the unit tests are most important to. As Neal Ford, co-author of _Fundamentals of Software Architecture_ and _Building Evolutionary Architectures_ says, "Testing is the engineering rigor of software development." 

The Test Spectrum also visually indicates that unit tests are just one piece of the testing puzzle. 

Without a solid suite of unit tests, software developers cannot have subsequent conversations about _code coverage_ -- how much of the codebase is _covered_ or tested by unit tests -- and _cyclomatic complexity_ -- how complex the codebase is, which can suggest that _hidden bugs_ might be masked by the accidental complexity of the code being tested.

These conversations are crucially important to me as a software developer, from a developer's perspective. But these tests don't speak to the user experience. Unit tests aren't shipped with the finished app. The user can't run them directly. While the user definitely benefits from a solid suite of unit tests in an abstract way, much like a symphonygoer benefits from a cellist applying bowstring wax before the performance and practicing their musical scales, the presence or absence of unit tests, let alone the intrinsic quality of them, is invisible to the end user.   

=== User Journey Testing with Taiko

So, what does speak to the user experience, and affect the user directly? The user interface, of course! From the user's perspective, the UI _is the app_, just like the API _is the app_ from the developer's perspective. The user isn't adding Strings to an Array, or even CatalogItems to a ShoppingCart object when they use the app -- they are adding tomatoes to their basket. 

And what might a test look like, from the user's perspective?

[code, javascript]
.Adding tomatoes to the basket with Taiko
----
openBrowser()
goto('https://thirstyhead.com/groceryworks/')
click('Produce')
click('Tomatoes')
click('Purchase')
----

These are the steps the user would take, quite possibly in a language similar to (but not identical to) what they would use to describe their user journey to someone else. The Taiko DSL isn't meant to be plain English, but hopefully it is readable to the non-programmer. 

Taiko is, in fact, well-formed JavaScript. It is an example of an _internal DSL_ -- "internal" to and consistent with the programming language that it is written in -- as opposed to an _external DSL_ which has its own personal syntax rules separate from its source programming language. 

If you want to capture a user journey in something even closer to the language the user used to describe the steps, you might be interested in another open source testing tool called _Gauge_. Gauge allows you to describe the steps of your test in a language called _Markdown_, which is as close to plain English as I've been able to find.

Here's what a Gauge test might look like:

[code, markdown]
.Adding tomatoes to the basket with Gauge
----
## Buying Bananas
* goto "GroceryWorks"
* click "Produce"
* click "Tomatoes"
* click "Purchase"
----

And here's another way that you could represent the same user journey in Gauge:

[code, markdown]
.Another way to add bananas to the basket with Gauge
----
## Buying Tomatoes
* visit the shopping website 
* click on the "Produce" menu item in the sidebar
* select "Tomatoes" from the list of produce items
* press the "Purchase" button on the shopping cart 
----

Gauge and Taiko work quite well together. All you have to do is associate the steps in Gauge with the underlying code in Taiko, and you have a set of user journey tests expressed in a language that any non-programming user should recognize and understand.

Since our focus here is on Taiko, I'll leave Gauge behind for the time being. But if Gauge looks interesting to you, I encourage you to learn more about it at https://gauge.org/. 

=== Placing User Journey Tests on the Test Spectrum

Regardless of which language we use to express our user journey tests, where do you think they might show up on the Test Spectrum?

.User journey tests on the Test Spectrum
image::introduction/dx-ux-2@2x.png[User journey tests on the Test Spectrum]

As you can see, unit tests and user journey tests both exercise the app in important ways. The only difference is that unit tests exercise the app from the developer's perspective, while user journey tests exercise the app from the user's perspective. Unit tests are written in the language of the developer, while user journey tests are written in the language of the user. Unit tests are for the benefit of the developer, while user journey tests are for the benefit of the user. 

And why didn't I place user journey tests to the far right of the Test Spectrum like I did with unit tests to the far left? Well, there are a number of important types of tests that aren't automated, or even automatable. _Manual tests_ live at the far right -- tests that are run by hand instead of by software. 

For example, _usability tests_ give the user a task to perform like, "Buy the ingredients you'd need to make a spaghetti dinner" while usability experts watch and evaluate how easy it is to complete the task. Another example is _accessibility tests_, where a user who has low vision or complete vision loss is encouraged to make the same user journey through the app to purchase the ingredients for a spaghetti dinner.

Since user journey tests are automated, they are one step closer to the developer experience than manual tests are. Similarly, integration tests are one step closer to the user experience than unit tests are. 

If you filled in the Test Spectrum with all of the puzzle pieces, it might look something like this:

.A variety of tests on the Test Spectrum
image::introduction/dx-ux-full@2x.png[A variety of tests on the Test Spectrum]

Note that this is highly subjective, and not meant to be a recipe for you to follow down to the letter. It's meant to be a way for you to think about tests. Is this particular type of test closer to the finished app, or farther away? Whose experience does this type of test affect most? 

For example, _fitness functions_ test the health of the application's architecture rather than a low-level API. If your app doesn't meet minimum _accessibility_ requirements, that's a showstopper bug that the developers need to fix. If it doesn't meet the minimum expectations you put into your _performance budget_, that is something the developers need to be notified of. These, along with _privacy_ and _security_, form the _APPS_ suite of fitness functions that I, as a Web Architect, typically put in place to ensure that the app in question is qualified and ready to be released to production. And just like unit tests, these fitness functions are opaque to the end user.

Now consider _A/B tests_ -- a programming technique that shows one version of a feature (A) to a select group of users, while another group of users are exposed to the (B) version of the same feature. An example of this might be allowing 1% of your user base to log in with their Twitter account (a new feature that you'd like to evaluate) while the remaining 99% log in with their existing username and password. Since this directly affects the user experience, I've placed it along the UX spectrum, but closest to the app and the developers. 

=== Empathy and the Test Spectrum

Another way that you can evaluate where the tests belong on the Test Spectrum is through the prism of _empathy_. When I'm in TDD-mode -- writing my tests first and watching 'em fail, then writing the code to make 'em pass -- I'm rarely testing for bugs. After all, the code doesn't exist yet! What kind of pessimist with low self esteem would I have to be to write tests while thinking, "Yup, this is the kind of bug I always write..."?  

Instead, I'm writing my first set of tests to take the API out for a spin. I'm quite literally the first user of my code when I write my tests, so I'm constantly asking myself, "How does this API feel? Did I name it well? Do the arguments make sense? Are they in a logical order?"

This is developer empathy. I'm putting myself in the shoes of other developers who will eventually use my code. How does that make me feel? Do I trust my code enough to share it with others? 

On the UX side of the equation, a set of practices called _Design Thinking_ resembles agile and XP practices in striking ways. Both are iterative processes. Both recommend making tiny changes, evaluating their effectiveness, and then repeating the process again.

And the first step in Design Thinking is empathy.

.Empathy is Stage 1 of the Design Thinking process (credit: https://www.interaction-design.org)
image::introduction/empathy-map@2x.png[Empathy is Stage 1 of the Design Thinking process]

So, when I'm writing user journey tests, I'm rarely looking for bugs, either. I'm taking the UI out for a spin. Are there too many clicks to get the bananas into the basket? Do I have to log in before I start putting things in my shopping cart, or can I log in just-in-time, right before I need to provide payment and shipping details? I ask myself, "Is this a user experience that I'd enjoy?" in the same way I ask myself if the API I'm writing is something that I'd enjoy as a developer. 

Just like I'd do with TDD, I can write my user journey tests before I've written the UI. I run the user journey test, and then watch it fail when I try to reach the website in question. I write just enough HTML to make the test pass, and then it fails when I try to click "Produce". I can add a "Produce" item to the sidebar to allow the next step to pass and keep going until I've successfully implemented the entire user journey. 

As a matter of fact, when I teach classes and workshops on web development, I've started including Taiko tests with each of the labs that I ask my students to complete. They can run the test at the beginning of the lab to see what I'm asking them to do, watch it fail, and then chip away at it until they know that they're unambiguously done and unambiguously correct.

Just like I've done for years with my other programming classes and unit test-driven labs.

The one thing that might feel odd about writing user journey tests, when compared to unit tests, is the apparent lack of assertions. At least, it certainly did to me initially.

When I'm teaching my students about unit tests, we focus on inputs, outputs, and assertions. If I add three things to my shopping cart in a test, my assertion might be something like `assert shoppingCart.itemCount() === 3`. Most people think of things like JUnit as a "testing library", while, in fact, it really is more of an "assertion library". 

There are a number of different testing/assertion libraries written in JavaScript, and you are welcome to incorporate any of them into your Taiko testing routine. After all, a Taiko test is just a well-formed JavaScript program that runs in Node.js. If I need an assertion library, I'll typically type `const assert = require('assert')` at the top of my Taiko test. But I do that less often than you might think.

Why is that? Because each step of a user journey test is an implicit assertion. I'm validating that the user journey works as I expect it to, and any step along the way that fails means that my assumption about the journey is flawed. 

If I'm adding bananas to my basket, I'm confident that some developer deep in the depths of that opaque codebase has written a unit test to make sure that `shoppingCart.itemCount()` is valid and ready to be used in production. With Taiko, I'm not testing the bricks; I'm testing the building.



